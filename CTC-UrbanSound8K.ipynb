{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wecooh6GZzN",
        "outputId": "9fc1ee5a-4265-4a91-901b-ade046f51d46"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Example: Using ResNet50 as the CNN backbone\n",
        "backbone = models.resnet50(pretrained=True)\n",
        "# Replace the final fully connected layer with an identity transform\n",
        "backbone.fc = torch.nn.Identity()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "THKlwp-WGgJc"
      },
      "outputs": [],
      "source": [
        "class ProjectionHead(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=512, output_dim=128):\n",
        "        super(ProjectionHead, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Example: Creating a projection head\n",
        "# Assuming the output features of your backbone has 2048 dimensions\n",
        "projection_head = ProjectionHead(input_dim=2048)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XxJFtcbtGiIg"
      },
      "outputs": [],
      "source": [
        "class NTXentLoss(torch.nn.Module):\n",
        "    def __init__(self, temperature, device):\n",
        "        super(NTXentLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.device = device\n",
        "        self.criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    def forward(self, z_i, z_j):\n",
        "        N, Z = z_i.size()  # Batch size and feature dimension\n",
        "\n",
        "        # Concatenate the positive pairs\n",
        "        z = torch.cat((z_i, z_j), dim=0)\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        sim = torch.mm(z, z.T) / self.temperature\n",
        "        sim_i_j = torch.diag(sim, N)\n",
        "        sim_j_i = torch.diag(sim, -N)\n",
        "\n",
        "        # Create positive and negative masks\n",
        "        positive_mask = torch.cat((sim_j_i, sim_i_j), dim=0).reshape(2 * N, 1)\n",
        "        negative_mask = sim > -1e6  # Mask to remove self-similarity\n",
        "\n",
        "        labels = torch.from_numpy(np.array([range(N), range(N)])).view(2 * N).to(self.device)\n",
        "        loss = self.criterion(sim, labels)\n",
        "\n",
        "        return loss\n",
        "\n",
        "# Example: Creating the NT-Xent Loss\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "contrastive_loss = NTXentLoss(temperature=0.5, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "T6Kjgc2gGieq"
      },
      "outputs": [],
      "source": [
        "class SimCLR(torch.nn.Module):\n",
        "    def __init__(self, backbone, projection_head):\n",
        "        super(SimCLR, self).__init__()\n",
        "        self.backbone = backbone\n",
        "        self.projection_head = projection_head\n",
        "\n",
        "    def forward(self, x_i, x_j):\n",
        "        h_i = self.backbone(x_i)\n",
        "        h_j = self.backbone(x_j)\n",
        "\n",
        "        z_i = self.projection_head(h_i)\n",
        "        z_j = self.projection_head(h_j)\n",
        "\n",
        "        return z_i, z_j\n",
        "\n",
        "# Instantiate the SimCLR model\n",
        "simclr_model = SimCLR(backbone, projection_head)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "PlnpZgPKWfLb"
      },
      "outputs": [],
      "source": [
        "# Add a linear layer for classification (after training SimCLR model)\n",
        "class Classifier(torch.nn.Module):\n",
        "    def __init__(self, feature_dim, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc = torch.nn.Linear(feature_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "num_classes = 10  # Set the number of classes in UrbanSound8K\n",
        "feature_dim= 2048\n",
        "classifier = Classifier(feature_dim, num_classes).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HNIxBkS5KDUt"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "def get_simclr_transformations(size, s=1):\n",
        "    \"\"\"Return a set of data transformations for SimCLR.\n",
        "\n",
        "    Args:\n",
        "    - size (int): Size of the square crop.\n",
        "    - s (float): Strength of color jitter, typically between 0.5 and 1.5.\n",
        "\n",
        "    Returns:\n",
        "    - A torchvision transforms module.\n",
        "    \"\"\"\n",
        "    color_jitter = transforms.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
        "\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        color_jitter,\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    return data_transforms\n",
        "\n",
        "# Example usage\n",
        "transform = get_simclr_transformations(size=224)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "BpMApxSvP4lG"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class UrbanSoundDataset(Dataset):\n",
        "    def __init__(self, root_dir, fold, csv_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.fold = fold\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        # Filter the annotations for the current fold\n",
        "        self.current_fold_annotations = self.annotations[self.annotations['fold'] == self.fold]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.current_fold_annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_filename = self.current_fold_annotations.iloc[idx]['slice_file_name']\n",
        "        img_path = os.path.join(self.root_dir, f'fold{self.fold}', img_filename)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.current_fold_annotations.iloc[idx]['classID']\n",
        "\n",
        "        # Apply the transformation twice to get two augmented versions of the same image\n",
        "        xi = self.transform(image)\n",
        "        xj = self.transform(image)\n",
        "\n",
        "        return xi, xj, label\n",
        "\n",
        "\n",
        "# Instantiate the dataset\n",
        "dataset = UrbanSoundDataset(root_dir='./archive/', fold=1, csv_file=\"./archive/UrbanSound8K.csv\", transform=transform)\n",
        "\n",
        "# DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFA3CB6KQrmx",
        "outputId": "61fecf23-0bcd-4b23-91bf-ede4c6253ad8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-21rMOpQ1ww",
        "outputId": "c83e9af1-5ec2-402e-bf20-b753e446f64d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting fold 1\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "expected scalar type Long but found Int",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[33], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m features \u001b[38;5;241m=\u001b[39m simclr_model\u001b[38;5;241m.\u001b[39mbackbone(xi)  \u001b[38;5;66;03m# Get features from one of the augmented images\u001b[39;00m\n\u001b[0;32m     48\u001b[0m classifier_output \u001b[38;5;241m=\u001b[39m classifier(features)\n\u001b[1;32m---> 49\u001b[0m loss_contrastive \u001b[38;5;241m=\u001b[39m \u001b[43mcontrastive_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# loss_contrastive = contrastive_loss(zi, zj)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# loss_classifier = torch.nn.functional.cross_entropy(classifier_output, labels)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# loss_classifier = torch.nn.functional.cross_entropy(classifier_output, labels.long())\u001b[39;00m\n\u001b[0;32m     54\u001b[0m loss_classifier \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(classifier_output, labels\u001b[38;5;241m.\u001b[39mlong())\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[28], line 24\u001b[0m, in \u001b[0;36mNTXentLoss.forward\u001b[1;34m(self, z_i, z_j)\u001b[0m\n\u001b[0;32m     21\u001b[0m negative_mask \u001b[38;5;241m=\u001b[39m sim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e6\u001b[39m  \u001b[38;5;66;03m# Mask to remove self-similarity\u001b[39;00m\n\u001b[0;32m     23\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mrange\u001b[39m(N), \u001b[38;5;28mrange\u001b[39m(N)]))\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m N)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 24\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Int"
          ]
        }
      ],
      "source": [
        "num_epochs = 15\n",
        "num_folds = 10\n",
        "root_dir = './archive/'\n",
        "feature_dim = 2048\n",
        "num_classes = 10\n",
        "batch_size = 32\n",
        "csv_file = \"./archive/UrbanSound8K.csv\"\n",
        "#base_lr = 0.3 * (batch_size / 256)  # Adjust batch_size according to your setup\n",
        "base_lr = 0.001\n",
        "weight_decay = 1e-6\n",
        "\n",
        "# Training and validation loop\n",
        "for fold in range(num_folds):\n",
        "    print(f\"Starting fold {fold+1}\")\n",
        "\n",
        "    # Setup training and validation data loaders\n",
        "    train_dataset = UrbanSoundDataset(root_dir=root_dir, fold=fold+1, csv_file=csv_file, transform=transform)\n",
        "    val_dataset = UrbanSoundDataset(root_dir=root_dir, fold=fold+1, csv_file=csv_file, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    # Initialize model, optimizer, and loss for each fold\n",
        "    simclr_model = SimCLR(backbone, projection_head).to(device)\n",
        "    classifier = Classifier(feature_dim, num_classes).to(device)\n",
        "    optimizer = torch.optim.Adam(list(simclr_model.parameters()) + list(classifier.parameters()), lr=0.001)#, weight_decay=weight_decay)\n",
        "    contrastive_loss = NTXentLoss(temperature=0.5, device=device)\n",
        "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0)\n",
        "\n",
        "    # Training loop for the current fold\n",
        "    for epoch in range(num_epochs):\n",
        "        simclr_model.train()\n",
        "        classifier.train()\n",
        "        train_loss = 0\n",
        "        \"\"\"\n",
        "        if epoch < 10:\n",
        "            lr_scale = min(1., float(epoch + 1) / 10.)\n",
        "            for pg in optimizer.param_groups:\n",
        "                pg['lr'] = lr_scale * base_lr\"\"\"\n",
        "\n",
        "        for (xi, xj, labels) in train_loader:\n",
        "            xi, xj, labels = xi.to(device), xj.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            zi, zj = simclr_model(xi, xj)\n",
        "\n",
        "            # Compute contrastive loss\n",
        "            features = simclr_model.backbone(xi)  # Get features from one of the augmented images\n",
        "            classifier_output = classifier(features)\n",
        "            loss_contrastive = contrastive_loss(zi.long(), zj.long())\n",
        "\n",
        "            # loss_contrastive = contrastive_loss(zi, zj)\n",
        "            # loss_classifier = torch.nn.functional.cross_entropy(classifier_output, labels)\n",
        "            # loss_classifier = torch.nn.functional.cross_entropy(classifier_output, labels.long())\n",
        "            loss_classifier = torch.nn.functional.cross_entropy(classifier_output, labels.long())\n",
        "\n",
        "            loss = loss_contrastive + loss_classifier\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            #scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Optional: Print the current learning rate\n",
        "        #current_lr = scheduler.get_last_lr()[0]\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss/len(train_loader)}\")\n",
        "        #print(f\"Epoch [{epoch+1}/{num_epochs}], Current LR: {current_lr}\")\n",
        "\n",
        "        # Validation step\n",
        "        simclr_model.eval()\n",
        "        classifier.eval()\n",
        "        val_accuracy = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0\n",
        "            for (xi, xj, labels) in val_loader:\n",
        "                xi, xj, labels = xi.to(device), xj.to(device), labels.to(device)\n",
        "                zi, zj = simclr_model(xi, xj)\n",
        "                loss_contrastive = contrastive_loss(zi, zj)\n",
        "\n",
        "                features = simclr_model.backbone(xi)\n",
        "                outputs = classifier(features)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                val_loss += loss.item()\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss/len(val_loader)}\")\n",
        "            accuracy = 100 * correct / total\n",
        "            print(f\"Fold {fold+1}, Validation Accuracy: {accuracy}%\")\n",
        "\n",
        "    # Save model after each fold\n",
        "    torch.save({'simclr_model': simclr_model.state_dict(),\n",
        "                'classifier': classifier.state_dict()},\n",
        "               f'simclr_classifier_urbansound8k_fold{fold+1}.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# logistic regression "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Training Loss: 1.6911799332925253\n",
            "Validation Accuracy: 36.99885452462772%\n",
            "Epoch [2/15], Training Loss: 0.9044251101357597\n",
            "Validation Accuracy: 47.99541809851088%\n",
            "Epoch [3/15], Training Loss: 0.6573553553649357\n",
            "Validation Accuracy: 52.00458190148912%\n",
            "Epoch [4/15], Training Loss: 0.523825249501637\n",
            "Validation Accuracy: 54.29553264604811%\n",
            "Epoch [5/15], Training Loss: 0.428718876093626\n",
            "Validation Accuracy: 55.670103092783506%\n",
            "Epoch [6/15], Training Loss: 0.367296858557633\n",
            "Validation Accuracy: 54.29553264604811%\n",
            "Epoch [7/15], Training Loss: 0.3491870766239507\n",
            "Validation Accuracy: 54.63917525773196%\n",
            "Epoch [8/15], Training Loss: 0.2965950902019228\n",
            "Validation Accuracy: 53.608247422680414%\n",
            "Epoch [9/15], Training Loss: 0.2929704732128552\n",
            "Validation Accuracy: 55.09736540664376%\n",
            "Epoch [10/15], Training Loss: 0.268234039789864\n",
            "Validation Accuracy: 55.09736540664376%\n",
            "Epoch [11/15], Training Loss: 0.22181155505989278\n",
            "Validation Accuracy: 56.93012600229095%\n",
            "Epoch [12/15], Training Loss: 0.23189489543437958\n",
            "Validation Accuracy: 55.09736540664376%\n",
            "Epoch [13/15], Training Loss: 0.21872090894196713\n",
            "Validation Accuracy: 56.013745704467354%\n",
            "Epoch [14/15], Training Loss: 0.18625371184732234\n",
            "Validation Accuracy: 57.9610538373425%\n",
            "Epoch [15/15], Training Loss: 0.17999803833663464\n",
            "Validation Accuracy: 57.044673539518904%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class UrbanSoundDataset(Dataset):\n",
        "    def __init__(self, root_dir, fold, csv_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.fold = fold\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.current_fold_annotations = self.annotations[self.annotations['fold'] == self.fold]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.current_fold_annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_filename = self.current_fold_annotations.iloc[idx]['slice_file_name']\n",
        "        img_path = os.path.join(self.root_dir, f'fold{self.fold}', img_filename)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.current_fold_annotations.iloc[idx]['classID']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
        "])\n",
        "\n",
        "dataset = UrbanSoundDataset(root_dir='./archive/', fold=1, csv_file=\"./archive/UrbanSound8K.csv\", transform=transform)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "input_dim = 2048  \n",
        "num_classes = 10  \n",
        "lr_model = LogisticRegression(input_dim, num_classes)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(lr_model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    lr_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        features = simclr_model.backbone(images)  \n",
        "        outputs = lr_model(features)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {total_loss/len(dataloader)}\")\n",
        "   \n",
        "    lr_model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            features = simclr_model.backbone(images)\n",
        "            outputs = lr_model(features)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Validation Accuracy: {accuracy}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n",
            "c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n",
            "c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n",
            "c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n",
            "c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n",
            "c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n",
            "c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n",
            "c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n",
            "c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n",
            "c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n",
            "c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n",
            "c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n",
            "c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n",
            "c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n",
            "c:\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 62.54295532646048%\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    svm_model = svm.LinearSVC()  \n",
        "    train_features = []\n",
        "    train_labels = []\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        features = simclr_model.backbone(images)\n",
        "        train_features.extend(features.detach().cpu().numpy()) \n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    svm_model.fit(train_features, train_labels)\n",
        "\n",
        "val_features = []\n",
        "val_labels = []\n",
        "\n",
        "for images, labels in val_loader:\n",
        "    images = images.to(device)\n",
        "\n",
        "    features = simclr_model.backbone(images)\n",
        "    val_features.extend(features.detach().cpu().numpy()) \n",
        "    val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "val_predictions = svm_model.predict(val_features)\n",
        "accuracy = (val_predictions == val_labels).mean() * 100\n",
        "print(f\"Validation Accuracy: {accuracy}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Validation Accuracy: 54.41008018327606%\n",
            "Epoch [2/15], Validation Accuracy: 50.40091638029782%\n",
            "Epoch [3/15], Validation Accuracy: 50.28636884306987%\n",
            "Epoch [4/15], Validation Accuracy: 51.20274914089347%\n",
            "Epoch [5/15], Validation Accuracy: 51.317296678121416%\n",
            "Epoch [6/15], Validation Accuracy: 50.85910652920962%\n",
            "Epoch [7/15], Validation Accuracy: 52.80641466208477%\n",
            "Epoch [8/15], Validation Accuracy: 52.80641466208477%\n",
            "Epoch [9/15], Validation Accuracy: 50.74455899198167%\n",
            "Epoch [10/15], Validation Accuracy: 52.119129438717074%\n",
            "Epoch [11/15], Validation Accuracy: 51.08820160366552%\n",
            "Epoch [12/15], Validation Accuracy: 53.264604810996566%\n",
            "Epoch [13/15], Validation Accuracy: 50.51546391752577%\n",
            "Epoch [14/15], Validation Accuracy: 53.264604810996566%\n",
            "Epoch [15/15], Validation Accuracy: 50.85910652920962%\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    xgb_model = XGBClassifier() \n",
        "    train_features = []\n",
        "    train_labels = []\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        features = simclr_model.backbone(images)\n",
        "        train_features.extend(features.detach().cpu().numpy())  \n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    xgb_model.fit(train_features, train_labels)\n",
        "\n",
        "    val_features = []\n",
        "    val_labels = []\n",
        "\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        features = simclr_model.backbone(images)\n",
        "        val_features.extend(features.detach().cpu().numpy())\n",
        "        val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_predictions = xgb_model.predict(val_features)\n",
        "    accuracy = (val_predictions == val_labels).mean() * 100\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {accuracy}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RFC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Validation Accuracy: 52.119129438717074%\n",
            "Epoch [2/15], Validation Accuracy: 54.41008018327606%\n",
            "Epoch [3/15], Validation Accuracy: 54.29553264604811%\n",
            "Epoch [4/15], Validation Accuracy: 53.95189003436426%\n",
            "Epoch [5/15], Validation Accuracy: 55.55555555555556%\n",
            "Epoch [6/15], Validation Accuracy: 54.86827033218786%\n",
            "Epoch [7/15], Validation Accuracy: 56.24284077892325%\n",
            "Epoch [8/15], Validation Accuracy: 54.06643757159221%\n",
            "Epoch [9/15], Validation Accuracy: 53.15005727376862%\n",
            "Epoch [10/15], Validation Accuracy: 53.95189003436426%\n",
            "Epoch [11/15], Validation Accuracy: 54.52462772050401%\n",
            "Epoch [12/15], Validation Accuracy: 51.890034364261176%\n",
            "Epoch [13/15], Validation Accuracy: 54.98281786941581%\n",
            "Epoch [14/15], Validation Accuracy: 56.013745704467354%\n",
            "Epoch [15/15], Validation Accuracy: 56.24284077892325%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    rf_model = RandomForestClassifier() \n",
        "    train_features = []\n",
        "    train_labels = []\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        features = simclr_model.backbone(images)\n",
        "        train_features.extend(features.detach().cpu().numpy()) \n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    rf_model.fit(train_features, train_labels)\n",
        "\n",
        "    val_features = []\n",
        "    val_labels = []\n",
        "\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        features = simclr_model.backbone(images)\n",
        "        val_features.extend(features.detach().cpu().numpy()) \n",
        "        val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_predictions = rf_model.predict(val_features)\n",
        "    accuracy = (val_predictions == val_labels).mean() * 100\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {accuracy}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Validation Accuracy: 68.8430698739977%\n",
            "Epoch [2/15], Validation Accuracy: 68.49942726231386%\n",
            "Epoch [3/15], Validation Accuracy: 66.09392898052691%\n",
            "Epoch [4/15], Validation Accuracy: 69.41580756013745%\n",
            "Epoch [5/15], Validation Accuracy: 70.2176403207331%\n",
            "Epoch [6/15], Validation Accuracy: 67.58304696449026%\n",
            "Epoch [7/15], Validation Accuracy: 69.18671248568155%\n",
            "Epoch [8/15], Validation Accuracy: 69.18671248568155%\n",
            "Epoch [9/15], Validation Accuracy: 68.38487972508591%\n",
            "Epoch [10/15], Validation Accuracy: 69.18671248568155%\n",
            "Epoch [11/15], Validation Accuracy: 69.3012600229095%\n",
            "Epoch [12/15], Validation Accuracy: 68.27033218785796%\n",
            "Epoch [13/15], Validation Accuracy: 67.12485681557845%\n",
            "Epoch [14/15], Validation Accuracy: 68.04123711340206%\n",
            "Epoch [15/15], Validation Accuracy: 67.92668957617411%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    knn_model = KNeighborsClassifier()\n",
        "    train_features = []\n",
        "    train_labels = []\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        features = simclr_model.backbone(images)\n",
        "        train_features.extend(features.detach().cpu().numpy())\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    knn_model.fit(train_features, train_labels)\n",
        "\n",
        "    val_features = []\n",
        "    val_labels = []\n",
        "\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        features = simclr_model.backbone(images)\n",
        "        val_features.extend(features.detach().cpu().numpy()) \n",
        "        val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "\n",
        "    val_predictions = knn_model.predict(val_features)\n",
        "    accuracy = (val_predictions == val_labels).mean() * 100\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Accuracy: {accuracy}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
